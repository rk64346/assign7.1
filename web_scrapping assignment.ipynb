{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9851828d-de1d-46f3-8ceb-f8d67770dec6",
   "metadata": {},
   "source": [
    "Q1-what is web scrapping?why is it used?give three areas where web scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f33eb0f-1ec8-4ac2-bd58-f421ceff4a68",
   "metadata": {},
   "source": [
    "Web scraping, also known as web harvesting or web data extraction, is the process of automatically extracting data from websites. It involves using software tools or scripts to access web pages, retrieve the desired information, and then save or process that information in a structured format. Web scraping is used to gather data from the internet, converting unstructured data into a structured format that can be analyzed, stored, or used for various purposes.\n",
    "\n",
    "Web scraping, also known as web harvesting or web data extraction, is the process of automatically extracting data from websites. It involves using software tools or scripts to access web pages, retrieve the desired information, and then save or process that information in a structured format. Web scraping is used to gather data from the internet, converting unstructured data into a structured format that can be analyzed, stored, or used for various purposes.\n",
    "\n",
    "Financial Data Analysis: Financial analysts and investors use web scraping to gather financial data such as stock prices, economic indicators, and corporate financial statements from various sources for analysis and decision-making.\n",
    "\n",
    "Healthcare Analytics: Web scraping can be used to gather data on patient outcomes, disease trends, and healthcare utilization from websites and databases. This data can be analyzed to identify patterns and insights that can guide healthcare policies and interventions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fabcb96-900e-4360-b930-265f8d8d69da",
   "metadata": {},
   "source": [
    "Q2-what are the different methods used for web scrapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fb594f-d31d-41fa-b541-1fe92c833f7e",
   "metadata": {},
   "source": [
    "HTTP Requests and HTML Parsing:\n",
    "\n",
    "HTTP Requests: Web scraping typically starts with sending HTTP requests to a website's server. These requests can be made using libraries like requests in Python. The server responds with the HTML content of the web page.\n",
    "HTML Parsing: Once you have the HTML content, you use an HTML parsing library like BeautifulSoup or lxml in Python to parse the HTML and navigate through its structure. This allows you to locate and extract the specific elements containing the data you need.\n",
    "XPath and CSS Selectors:\n",
    "\n",
    "XPath: XPath is a language used to navigate XML and HTML documents. It provides a way to select elements based on their attributes and relationships within the document. Libraries like lxml and xml.etree.ElementTree in Python can be used for XPath-based parsing.\n",
    "CSS Selectors: CSS selectors are patterns used to select HTML elements based on their attributes and positions in the document. Libraries like BeautifulSoup and pyquery provide support for using CSS selectors.\n",
    "APIs and Web Services:\n",
    "\n",
    "Some websites provide APIs (Application Programming Interfaces) that allow you to access their data in a structured way. Instead of scraping the HTML, you make requests to the API endpoints and receive data in formats like JSON or XML. This method is more reliable and efficient.\n",
    "APIs often require authentication through API keys or tokens to ensure proper access and usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51414e79-c149-4c55-a7c3-27dc405a06c2",
   "metadata": {},
   "source": [
    "Q3- what is BeutifulSoup? why it is used ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa60688-e3cc-4ec3-8971-36938aa16ceb",
   "metadata": {},
   "source": [
    "Certainly! Beautiful Soup is a Python library that transforms the raw and sometimes messy data collected through web scraping into an organized and easily understandable structure. It's like a digital organizer for the chaotic data found on web pages. With Beautiful Soup, you can neatly arrange the scraped content, making it more readable and navigable for both humans and computers.\n",
    "\n",
    "Think of Beautiful Soup as a translator that takes the jumbled text and tags from a webpage and organizes them into a well-structured format. This format resembles a tree, where each branch represents an element, and each node holds relevant data. This organization not only makes the data human-readable but also makes it much simpler to extract specific pieces of information from the web page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c084ef0-e189-4563-9f43-99a7e8044d6e",
   "metadata": {},
   "source": [
    "Q4- why flask is used in the web scrapping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe45cc-fb1c-476a-bf43-571f78e52fb1",
   "metadata": {},
   "source": [
    "While Flask is primarily known for building web applications, it can also be a valuable component in web scraping projects. Here are some reasons why Flask might be used in a web scraping project:\n",
    "\n",
    "API Development: Flask can be used to create a RESTful API that exposes the scraped data. This allows other applications or clients to easily retrieve the data in a structured format like JSON. This is especially useful if you want to share or distribute the scraped data to other systems.\n",
    "\n",
    "User Interface: If you want to provide a user interface to interact with the scraping process or to display the scraped data, Flask can be used to build a simple web interface. Users can input parameters, initiate the scraping process, and view the results through a web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac447f98-295b-4fb7-a212-ad8649327fc9",
   "metadata": {},
   "source": [
    "Q5- write the names of AWS service used in this project.also,explain the uses of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62611408-c0c6-485d-8eae-c9a8aac33e18",
   "metadata": {},
   "source": [
    "AWS CodePipeline:\n",
    "AWS CodePipeline is a continuous integration and continuous delivery (CI/CD) service provided by Amazon Web Services. It allows you to automate and streamline the process of building, testing, and deploying your code changes to production. Here's how it works:\n",
    "\n",
    "Pipeline Configuration: You set up a pipeline that defines the stages your code goes through before it reaches production. Common stages include source code retrieval, building, testing, and deployment.\n",
    "\n",
    "Source Integration: CodePipeline integrates with various source code repositories, including GitHub. When you commit changes to your GitHub repository, CodePipeline can automatically detect these changes and trigger the pipeline.\n",
    "\n",
    "Automated Builds and Tests: Once triggered, CodePipeline can initiate automated processes, like building your application and running tests to ensure its quality.\n",
    "\n",
    "Deployment: After the build and tests are successful, CodePipeline can deploy your application to your chosen deployment environment, which could be Amazon EC2 instances, AWS Elastic Beanstalk, AWS Lambda, or other services.\n",
    "\n",
    "Monitoring and Notifications: CodePipeline provides monitoring of each stage and can send notifications in case of failures or issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
